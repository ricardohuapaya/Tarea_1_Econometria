---
title: "Solución Tarea 2"
author: "Integrantes: María Fernanda Villalobos Barboza B78304, \nRicardo Huapaya Rey B83936, \nSebasthián Valverde Martínez B98083, \nSebastian Mora Rojas B95205\n"
date: '`r format(Sys.Date(), "%d %b, %Y")`'
output:
  html_document: default
  word_document: default
theme: "journal"
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, echo=FALSE, results='hide', message=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(readxl)
library(dplyr)
library(tseries)
library(stargazer)
library(normtest)
library(skedastic)
library(car)
library(olsrr)
library(lmtest)
library(sandwich)

setwd("~/UCR/Tarea_2_Econometria")

cigs = read_excel("Data/CigarettesB.xlsx")

energy = read_excel("Data/Electricity1970.xlsx")

house = read_excel("Data/HousePrices.xlsx")

datos_reg <- select(energy, Costo= ln_COST, Produccion = ln_output, 
                    Trabajo= ln_labor, Capital= ln_capital, Combustible= ln_fuel)
```

En este presente documento se procede a realizar un análisis econométrico de diversas bases de datos de interés.
Para ello se selecionaron las bases:

-   2.2 CigarettesB: Cigarette Consumption Data

-   2.6 Electricity1970: Cost Function of Electricity Producers 1970

-   2.10 HousePrices: House Prices in the City of Windsor, Canada.

La estructura del documento es de un análisis individual en el orden mencionado previamente.
Para así al final discutir en una última sección las conclusiones de cada análisis.
El codigo del codigo se encuentra en el siguiente [repositorio](https://github.com/ricardohuapaya/Tarea_1_Econometria).

## 1. Análisis Cigarettes Data

Para la base de datos 2.2 tomaremos la variable `PACKS` como nuestra variable dependiente que será analizada.
Esta variable representa el número de paquetes consumidos por personas mayores de 16 años en casa Estado; así nuestro modelo propuesto para el primer análisis sería de la forma:

$$
ln(PACKS) = \beta_{0}+\beta_{1}ln(price)+\beta_{2}income+\mu
$$

### 1.1 Regresión

Con el fin de analizar la regresión lineal de nuestras variables, utilizaremos el siguiente código:

```{r, warning = FALSE}
linear.cigs <-lm(PACKS ~ price+income, data=cigs)
```

La regresión nos muestra que en promedio se consumiría $4.3$ paquetes de cigarillos si el precio y los ingresos fueran $0$.
En este otro caso note que dada la definición del modelo $\beta_{1}$ es un indicador de Elasticidad precio de la demanda de cigarrillos.
Considere que al ser mayor a uno no sigue la intuición económica incial que los cigarrillos son ineslásticos.

```{r, echo=TRUE, warning=FALSE}
stargazer(linear.cigs, type = "text", title = "Cuadro #1.1: Resultados Regresión")
coef = coefficients(linear.cigs)
```

### 1.2 Pruebas t

Utilizando la misma función que en la sección anterior, y con el cuadro 1 se obtiene la información necesaria para obtener los resultados de la prueba $t$.

Tomando $H_{0}: \beta_{1}=0$ (Precio no tiene efecto en consumo) La prueba $t$ realizada para el precio nos devuelve un P-value de 0.000168, entonces con un nivel de significancia del 5% podemos rechazar la hipotesis nula donde el precio del paquete no afecta el consumo.

Tomando $H_{0}: \beta_{2}=0$ (Ingreso no tiene efecto en consumo) La prueba t realizada para el ingreso nos devuelve un P-Value de 0.386 el cual, entonces con un nivel de significancia del 5% podemos no podemos rechazar la hipotesis nula donde el precio del paquete no afecta el consumo.

### 1.3 Pruebas F

Utilizando la misma función que en la pregunta anterior, y con el cuadro 1 se obtiene la información necesaria para obtener los resultados de la prueba $f$.

Tomando una prueba de significancia conjunta tal que nuestra hipotesis nula sea; $H_{0}: \beta_{1}= \beta_{2}=0$.
La prueba $f$ conjunta nos devuelve un P-value de 0.0004168, entonces con un nivel de significancia del 5% se rechaza la hipotesis nula conjunta de que el precio y el ingreso no afectan el consumo.

### 1.4 Prubas Generales Evaluación de Supuestos MCRL

Para esta sección procedemos a analizar los resultados de la secciones 1.1-1.3, para ello consideraremos los resultados y en la siguiente sección procederemos a intentar corregir los efectos mostrados o justificar las acciones pasivas frente a ellos.

#### Jarque-Bera

Considere la prueba de normalidad Jarque-Bera, para ello podemos observar inicialmente la distribución de los residuos de nuestra regresión `linear.cigs`.
Note que empiricamente, aparenta tener una distribución normal.

```{r Historgram, echo=FALSE, message=TRUE}

hist(residuals(linear.cigs))
```

Con esta información podemos correr la prueba de Jarque-Bera, para ello parta de la siguiente hipotesis nula donde,

$$
H_{0}: JB\sim \overline{\chi^{2}}_{2 \, g.l.}
$$

```{r Jarque-Bera, echo=FALSE}

jarque.bera.test(residuals(linear.cigs))
```

El codigo nos da un resultado de con el p-value de 0.86, por lo que con un nivel de significancia del 5% no podemos rechazar $H_{0}$.
Por ello concluimos que los residuos se distribuyen de forma normal.

#### Prueba de Heterocedasticidad (White)

Considere ahora una prueba de Heterocedasticidad, en este caso al ser dos variables explicativas se puede aplicar la regresión la prueba de White con **interacciones**.
Para ello la hipotesis nula se especifica como,

$$
H_{0}: \nexists \, Heterocedasticidad
$$

Corremos la prueba la cual nos da el siguiente resultado,

```{r White, echo=FALSE}
white.cigs <- as.data.frame(white_lm(linear.cigs, interactions = TRUE))

stargazer(white.cigs, type ="text",title = "Cuadro #1.2: Prueba de Heterocedastidad", summary = FALSE)

```

Observando los resultados de la prueba se obtiene que con un nivel de significancia del 5%, se tiene un p-value de 0.008 por lo que Rechazamos la hipotesis nula y confirmamos que sí existe la presencia de heterocedasticidad en el modelo.

#### Prueba de Multicolinealidad

Inicialmente considere dentro de nuestra correlación si ha de existir un problema de multicolinealidad perfecta, corremos un codigo en el modelo para observar si este detecta algun error.

```{r}
linear.cigs <-lm(PACKS ~ price+income, data=cigs, singular.ok = FALSE)
```

Note que por el momento no tenemos un problema de multicolinealidad perfecta.
Sin embargo, es necesario realizar un analisis más de este.
Considere un analisis de multicolinealidad, para ello considere los siguientes factores, la $R^{2}$ a un nivel de, vista en el Cuadro \#1; y la matriz de correlaciones entre variables explicativas.

```{r, echo=TRUE}


cor(cigs$price, cigs$income, method="pearson")
cor(cigs$price, cigs$PACKS, method="pearson")
cor(cigs$income, cigs$PACKS, method="pearson")

```

Visto los resultados de la matriz, vemos que si existe alta correlación entre varias variables, sin embargo, nada mayor a 8.
Por ellos ahora usaremos el analisis del indice K.

Procedemos en analsis del indice K donde sabemos que,

$$
K= \frac{\max_{valor \, propio}X'X}{\min_{valor \, propio}X'X}
$$

Donde el Indice K (IC) es $\sqrt{K}$, donde si

-   Si 10 \< IC \< 30 hay multicolinealidad modera fuerte.

-   Si IC\> 30 hay un problema serio de multicolinealidad.

```{r}

#Se crea la matriz de variables explicativas

X <- as.matrix(cbind(cigs$price,cigs$income))

#Se computa X'X

X_prima_X <- t(X)%*%X

#Se obtienen los valores propis de la matriz X'X con la función eigen (base de R), se usa la opción only.values = TRUE para que solo brinde los valores propios y no sus vectores asociados
valores_propios<-eigen(X_prima_X, only.values = TRUE)

#Se computa el índice de condición K
indice_condicion_k<-sqrt(max(valores_propios$values))/sqrt(min(valores_propios$values))

indice_condicion_k
```

El Indice K nos da un númeri superior a 30, por lo que se tiene un problema serio de multicolinealidad.

#### Prueba de Autocorrelación (Durbin y Watson)

Vamos a realizar finalmente una prueba que estudie la correlación de nuestro modelo, para ello recuerde la formula de estadistico $d$

$$
d=\frac{\sum_{t=2}^{T}(\hat{u_{t}}-\hat{u_{t-1}})^{2}}{\sum_{t=1}^{T}\hat{u_{t}^{2}}}
$$

Para ello considere la siguiente hipotesis nula,

$$
H_{0}: d=2 \implies \nexists \, autocorrelación
$$

```{r, echo=FALSE}

durbinWatsonTest(linear.cigs)


```

Para este caso con $n=46$ y $k=2$, tenemos que dentro de nuestro mapa.

$$
d_{L} = 1.48 \, \& \, d_{U} = 1.56 
$$

Como nuestro estadistico nos dio 2.32, note que $2.32 < (4-1.56 )$, asi entonces no se puede rechazar la hipotesis nula y concluimos que no hay presencia de autocorrelación.

### 1.5 Corrección a las Pruebas

Dado que nuestro modelo sufre de un problema moderado de multicolinealidad y un problema de heteroscedasticidad entre sus residuos, aplicamos ciertas medidas para corregir estos problemas.
La principal medida que vamos a aplicar es la eliminación de la variable 'Income' y convertir la regresión en una regresión simple de la forma: $$
ln(PACKS) = \beta_{0}+\beta_{1}price+\mu
$$

A pesar de que el ingreso es una variable que pueda explicarnos a cierto nivel la demanda por cigarillos, en este caso no es estadisticamente significativo y solo nos genera suciedad en la regresión.
Adicionalmente, en un sentido economico, la demanda de los cigarillos no tienden a ser influeciados por el ingreso que los individuos tienen, por su naturaleza son un bien "necesario" que se cosume irrelevantemente del ingreso de las personas.

Nuestra nueva regresión quedaría de la la forma.

```{r, warning = FALSE}
linear.cigs.corr <-lm(PACKS ~ price, data=cigs)
```

#### Multicolinealidad

Por definicion una regresión simple no puede tener multicolinealidad, ya que no hay otras variables con las cuales "Price" pueda tener alguna asociacion lineal con, el remover la variable de Income nos soluciona el problema de multicolinealidad.

#### Heteroscedasticidad

Para la prueba de Hetereocedasticidad, vemos en este caso que dicho cambio mejoró el resultado de nuestro modelo.

```{r echo=FALSE, warning=FALSE}
whitecig2=as.data.frame(white_lm(linear.cigs.corr))

stargazer(whitecig2, type ="text", title = "Cuadro #2.1: Prueba de Heterocedastidad", summary = FALSE)
```

Observando el resultado se obtiene que con un nivel de significancia de 5% una vez no se rechaza la hipotesis nula, confirmando que existe no heteroscedasticidad en el modelo corregido.

### 1.6 Comparación

```{r}
stargazer(linear.cigs,linear.cigs.corr,type="text", title="Cuadro #2.2: Original vs Corregidos")
```

En conclusión, para nuestra regresión

## 2. Análisis Energy Data

Para la base de datos 2.6 tomaremos el la variable `Costo` como nuestra variable dependiente que será analizada.
Esta variable representa el costo en dolares por cada mil kwh.
Las variables explicativas tomadas son el salario promedio de la firma, el indice de capital, el precio de combustible y el output final en millones de kwh.
A partir de estas variables, la regresión sera calculada con los logaritmos naturales de las variables costo, producción, trabajo y capital

$$
ln(Costo) = \beta_{0}+ \beta_{1}ln(producción)+\beta_{2}ln(trabajo)+\beta_{3}ln(capital)+\beta_{4}ln(combustible) +\mu
$$

### 2.1 Regresión Lineal

Con el fin de analizar la regresión lineal de nuestras variables, utilizaremos el siguiente codigo:

```{r, warning = FALSE}
linear.energy <- lm( Costo ~ + Trabajo + Capital + Combustible, data= datos_reg)
```

```{r, echo=FALSE, warning=FALSE}
stargazer(linear.energy, type = "text", title = "Cuadro #2.1: Resultados Regresión")
coef2 = coefficients(linear.energy)
```

Con todo lo demas constante se pueden realizar las siguientes interpretacions de los coeficientes.

Para el $\beta_{1}$ se interpreta como la elasticidad del costo a producción, si la producción incrementa en 1% se espera que los costos aumenten en 0.84%.

Para el $\beta_{3}$ se interpreta como la elasticidad del costo a capital, si el capital incrementa en 1% se espera que los costos aumenten en 0.18%.

Para el $\beta_{4}$ se interpreta como la semielasticidad del costo respecto al combustible, si la producción incrementa en un dolar se espera que los costos aumenten en 0.023%

### 2.2 Pruebas t

Utilizando la misma función que en la pregunta anterior, y con el cuadro 2 se obtiene la información necesaria para obtener los resultados de la prueba $t$.
Tomando $H_{0}: \beta_{1}=0$ (Producción no tiene efecto en los Costos) La prueba $t$ realizada para la producción nos devuelve un P-value que tiende a 0, entonces con un nivel de significancia del 5% podemos rechazar la hipotesis nula donde la producción no afecta los costos.

Tomando $H_{0}: \beta_{2}=0$ (Trabajo no tiene efecto en los Costos) La prueba $t$ realizada para la producción nos devuelve un P-value de 0.7515, entonces no se se puede rechazar la hipotesis nula, por lo tanto el trabajo no tiene efecto sobre los costos.

Tomando $H_{0}: \beta_{3}=0$ (Capital no tiene efecto en los Costos) La prueba $t$ realizada para la producción nos devuelve un P-value de 0.0496, entonces con un nivel de significancia del 5% podemos rechazar la hipotesis nula donde el capital no afecta los costos.

Tomando $H_{0}: \beta_{4}=0$ (Combustible no tiene efecto en los Costos) La prueba $t$ realizada para la producción nos devuelve un P-value que tiende a 0, entonces con un nivel de significancia del 5% podemos rechazar la hipotesis nula donde la producción no afecta los costos.

### 2.3 Prueba F

Utilizando la misma función que en la pregunta anterior, y con el cuadro 2 se obtiene la información necesaria para obtener los resultados de la prueba F.
Tomando una prueba de significancia conjunta tal que nuestra hipotesis nula sea; $H_{0}: \beta_{1}= \beta_{2}=\beta_{3}= \beta_{4}=0$.
La prueba F conjunta nos devuelve un P-value que tiende a 0, entonces con un nivel de significancia del 5% se rechaza la hipotesis nula conjunta de que los B´s no afectan los costos.

Interpretando la prueba F de significancia conjunta del modelo con la función summary donde la Ho: B´s(en conjunto)= 0 se puede afirmar que los coeficientes de las variables explicativas no son 0 ya que el p-value asociado a la prueba es practicamente 0.

### 2.4 Prubas Generales Evaluación de Supuestos MCRL

Para esta sección procedemos a analizar los resultados de la secciones 2.1-2.3, para ello consideraremos los resultados y en la siguiente sección procederemos a intentar corregir los efectos mostrados o justificar las acciones pasivas frente a ellos.

#### Jarque-Bera

Considere la prueba de normalidad Jarque-Bera, para ello podemos observar inicialmente la distribución de los residuos de nuestra regresión `linear.enerfy`.
Note que empiricamente, no aparenta tener una distribución normal.

```{r Historgram 2, echo=FALSE, message=TRUE}

hist(residuals(linear.energy))
```

Con esta información podemos correr la prueba de Jarque-Bera, para ello parta de la siguiente hipotesis nula donde,

$$
H_{0}: JB\sim \overline{\chi^{2}}_{2 \, g.l.}
$$

```{r Jarque-Bera 2, echo=FALSE}

jarque.bera.test(residuals(linear.energy))
```

El codigo nos da un resultado de con el p-value que tiende a 0, por lo que con un nivel de significancia del 5% podemos rechazar $H_{0}$.
Por ello concluimos que los residuos no se distribuyen de forma normal.

#### Prueba de Heterocedasticidad (White)

Considere ahora una prueba de Heterocedasticidad, en este caso al ser más de dos variables explicativas se debería aplicar la regresión la prueba de White sin **terminos cruzados**.
Para ello la hipotesis nula se especifica como,

$$
H_{0}: \nexists \, Heterocedasticidad
$$

Corremos la prueba la cual nos da el siguiente resultado,

```{r White 2, echo=FALSE}
white.energy <- as.data.frame(white_lm(linear.energy, interactions = FALSE))

stargazer(white.energy, type ="text",title = "Cuadro #2.2: Prueba de Heterocedastidad", summary = FALSE)


```

Observando los resultados de la prueba se obtiene que con un nivel de significancia del 5%, a partir de la prueba de White sin terminos cruzados se obtiene un p-value que tiende a 0 por lo que se rechaza la $H_{0}$ y se confirma la evidencia de heterocedasticidad.

#### Prueba de Multicolinealidad

Inicialmente considere dentro de nuestra correlación si ha de existir un problema de multicolinealidad perfecta, corremos un codigo en el modelo para observar si este detecta algun error.

```{r, echo=TRUE}
lm( Costo ~ Trabajo + Capital + Combustible, data= datos_reg, singular.ok = FALSE)
```

Note que por el momento no tenemos un problema de multicolinealidad perfecta.
Sin embargo, es necesario realizar un analisis más de este.
Considere un analisis de multicolinealidad, para ello considere los siguientes factores, la $R^{2}$ a un nivel de 0.98, vista en el Cuadro \#3; y la matriz de correlaciones entre variables explicativas.

```{r, echo=TRUE}

cor(datos_reg$Produccion, datos_reg$Trabajo ,method="pearson")
cor(datos_reg$Produccion, datos_reg$Capital ,method="pearson")
cor(datos_reg$Produccion, datos_reg$Combustible ,method="pearson")
cor(datos_reg$Produccion, datos_reg$Costo ,method="pearson")


```

Visto los resultados de la matriz, vemos que si existe naja correlación entre varias variablesPor ellos ahora usaremos el analisis del indice K.

Procedemos en analsis del indice K donde sabemos que,

$$
K= \frac{\max_{valor \, propio}X'X}{\min_{valor \, propio}X'X}
$$

Donde el Indice K (IC) es $\sqrt{K}$, donde si

-   Si 10 \< IC \< 30 hay multicolinealidad modera fuerte.

-   Si IC\> 30 hay un problema serio de multicolinealidad.

```{r}

#Se crea la matriz de variables explicativas

X <- as.matrix(cbind(datos_reg$Produccion, datos_reg$Trabajo, datos_reg$Capital, datos_reg$Combustible))

#Se computa X'X

X_prima_X <- t(X)%*%X

#Se obtienen los valores propis de la matriz X'X con la función eigen (base de R), se usa la opción only.values = TRUE para que solo brinde los valores propios y no sus vectores asociados
valores_propios<-eigen(X_prima_X, only.values = TRUE)

#Se computa el índice de condición K
indice_condicion_k<-sqrt(max(valores_propios$values))/sqrt(min(valores_propios$values))

indice_condicion_k
```

El Indice K nos da un número extremadamente alto, por lo que se tiene un problema muy serio de multicolinealidad.

#### Prueba de Autocorrelación (Durbin y Watson)

Vamos a realizar finalmente una prueba que estudie la correlación de nuestro modelo, para ello recuerde la formula de estadistico $d$

$$
d=\frac{\sum_{t=2}^{T}(\hat{u_{t}}-\hat{u_{t-1}})^{2}}{\sum_{t=1}^{T}\hat{u_{t}^{2}}}
$$

Para ello considere la siguiente hipotesis nula,

$$
H_{0}: d=2 \implies \nexists \, autocorrelación
$$

```{r, echo=FALSE}

durbinWatsonTest(linear.energy)


```

Para este caso con $n=158$ y $k=4$, tenemos que dentro de nuestro mapa.

$$
d_{L} = 1.70 \, \& \, d_{U} = 1.78
$$

Como nuestro estadistico nos dio 1.45, note que $1.45 < 1.70 )$, asi entonces se puede rechazar la hipotesis nula y concluimos que hay presencia de autocorrelación.

### 2.5 Corrección a las Pruebas

Se procede a realizar la correción del modelo enfocandonos en disminuir la heterocedasticidad.
Para ellos tomamos la el metodo de Minimos Cuadrados Ponderados.

```{r}

datos_reg$u2 <- abs(residuals(linear.energy))

linear.energy.corre <- lm( Costo ~ Produccion+ Trabajo + Capital + Combustible , data= datos_reg, weight=u2)


summary(linear.energy.corre)
```

Se puede observar a tráves de la función summary que cada variable explicativa es significante ya que los p-values asociados son practicamente 0 a comparación de la regresión original donde el componente del Trabajo no era estadisticamente significativo.
La prueba F conjunta nos devuelve un P-value que tiende a 0, entonces se sigue rechazando la $H_{0}: \beta_{1}= \beta_{2}=\beta_{3}= \beta_{4}=0$.

### 2.6 Comparación

```{r}
jarque.bera.test(residuals(linear.energy.corre))
hist(residuals(linear.energy.corre))


white_lm(linear.energy.corre, interactions = FALSE)





#Se crea la matriz de variables explicativas

X <- as.matrix(cbind(datos_reg$Produccion, datos_reg$Trabajo, datos_reg$Capital, datos_reg$Combustible))

#Se computa X'X

X_prima_X <- t(X)%*%X

#Se obtienen los valores propis de la matriz X'X con la función eigen (base de R), se usa la opción only.values = TRUE para que solo brinde los valores propios y no sus vectores asociados
valores_propios<-eigen(X_prima_X, only.values = TRUE)

#Se computa el índice de condición K
indice_condicion_k<-sqrt(max(valores_propios$values))/sqrt(min(valores_propios$values))

indice_condicion_k



```

A modo de comparación el modelo corregido presenta residuos que se distribuyen de manera normal ya que presenta un p-value de 0.4 por lo que no se rechaza la Ho, caso contrario al modelo original donde hay evidencia estadistica de residuos no normalizados En cuanto a la heterocedasticidad el p-value asociado al modelo corregido aumenta a 0.02 por lo que con un nivel de significancia del 1% no rechazamos Ho donde es que no exista heterocedasticidad; sin embargo no es algo con lo que estemos tranquilos pero lo consideramos aceptable en términos del problema y de la estructura que presentan los datos\
Por último el problema de multicolinealidad no se corrige con el nuevo modelo; sin embargo desde un punto económico y en terminos de los datos de la regresión se espera que exista una multicolinealidad fuerte entre estas variables explicativas; es esperable que la produccion, trabajo, capital, combustible mantengan una relación lineal que si no es perfecta al menos exista y nos genere ruido en la regresión.

## 3. Análisis Housing Data

Para la base de datos 2.10 tomaremos el la variable `PRICE`como nuestra variable dependiente que será analizada.
Esta variable representa el costo en dolares de las viviendas.
Las variables explicativas tomadas son el numero de cuartos, el tamaño del terreno y la cantidad de baños.

$$
Price = \beta_{0}+ \beta_{1}lotsize+\beta_{2}bedrooms+\beta_{3}bathrooms+\mu
$$

### 3.1 Regresión Lineal

Con el fin de analizar la regresión lineal de nuestras variables, utilizaremos el siguiente codigo:

```{r, warning = FALSE}
linear.house <- lm( PRICE ~ lotsize + bedrooms + bathrooms, data= house)
```

```{r, echo=FALSE, warning=FALSE}
stargazer(linear.house, type = "text", title = "Cuadro #3.1: Resultados Regresión")

```

Con todo lo demás constante se pueden realizar las siguientes interpretacions de los coeficientes.

Para el $\beta_{0}$ (el intercepto), se interpreta como el precio que existe al ser las variables explicativas iguales a cero.
Se observa que, el precio es negativo cuando las variables explicativas son cero.
No tiene mucho sentido económico, puesto que no hay casas en venta con precios negativos en el mercado.

Para el $\beta_{1}$ se interpreta como el aporte que genera el incremento de un metro cuadrado de la variable "lotsize" al precio de las casas.
Se ve que afecta positivamente al precio.

Para el $\beta_{2}$ se interpreta como el aporte que genera el incremento de una unidad de la variable "bedrooms" (número de habitaciones) al precio de las casas.
Igual que el tamaño del lote, afecta positivamente al precio.

Para el $\beta_{3}$ se interpreta como el aporte que genera el incremento de una unidad de la variable "bathrooms" (número de baños) al precio de las casas.
Se ve que, al igual que "lotsize" y "bedrooms" afecta positivamente al precio, pero, el número de baños afecta en una cuantía mayor al precio.
Es decir (comparándolo con las habitaciones), es más caro un baño adicional que una habitación adicional.

### 3.2 Pruebas t

Utilizando la misma función que en la pregunta anterior, y con el cuadro 5 se obtiene la información necesaria para obtener los resultados de la prueba $t$.

Tomando $H_{0}: \beta_{0}=0$ (El intercepto es nulo) La prueba $t$ realizada para el intercepto, nos devuelve un P-value que es aproximadamente, 0.523.
Entonces con un nivel de significancia del 5% podemos no rechazar la hipotesis nula donde el intercepto no afecta los precios, puesto que el P-value es mayor que el nivel de tolerancia a cometer el error tipo I.
Esto confirma la intuición del inciso pasado, en el que se decía que, no tendría mucho sentido que eso fuese cierto (que el precio de las casas fuesen negativos)

Tomando $H_{0}: \beta_{1}=0$ (El tamaño del lote no tiene efecto sobre el precio de las casas) La prueba $t$ realizada para el tamaño de los lotes nos devuelve un P-value muy cercano a cero.
Entonces se puede rechazar la hipotesis nula.
Es decir, el tamaño del lote sí tiene un efecto sobre el precio de las casas.

Tomando $H_{0}: \beta_{2}=0$ (El número de habitaciones no tiene un efecto en el precio de las casas).
La prueba $t$ realizada para el número de habitaciones nos devuelve un P-value cercano a cero.
Entonces con un nivel de significancia del 5% podemos rechazar la hipotesis nula donde el número de habitaciones no afectan los precios de las casas.
Es decir, el número de habitaciones sí afecta al precio de las casas.

Tomando $H_{0}: \beta_{3}=0$ (Número de baños no tiene efecto en los precios) La prueba $t$ realizada para el número de baños nos devuelve un P-value que tiende a 0.
Entonces con un nivel de significancia del 5% podemos rechazar la hipotesis nula donde el número de baños no afecta los precios de las casas.

### 3.3 Prueba F

Utilizando los datos de la regresión incluidos el cuadro 5 se obtiene la información necesaria para obtener los resultados de la prueba F.
Tomando una prueba de significancia conjunta tal que nuestra hipotesis nula sea; $H_{0}: \beta_{0}= \beta_{1}=\beta_{2}= \beta_{3}=0$.

La prueba F conjunta nos devuelve un P-value que tiende a 0.
Entonces dado lo anterior y con un nivel de significancia del 5% se rechaza la hipotesis nula de que los Betas de forma conjunta no afectan los precios de las casas.

Más claro: interpretando la prueba F de significancia conjunta del modelo con la función summary donde la Ho: Betas (en conjunto) = 0, se puede afirmar que los coeficientes de las variables explicativas no son cero, ya que el p-value asociado a la prueba es practicamente cero.

### 3.4 Prubas Generales Evaluación de Supuestos MCRL

Para esta sección procedemos a analizar los resultados de la secciones 3.1-3.3, para ello consideraremos los resultados y en la siguiente sección procederemos a intentar corregir los efectos mostrados o justificar las acciones pasivas frente a ellos.

#### Jarque-Bera

Considere la prueba de normalidad Jarque-Bera, para ello podemos observar inicialmente la distribución de los residuos de nuestra regresión `linear.house`.
Note que empiricamente, no aparenta tener una distribución normal.

```{r Historgram 3, echo=FALSE, message=TRUE}

hist(residuals(linear.house)/50000)
```

Con esta información podemos correr la prueba de Jarque-Bera, para ello parta de la siguiente hipotesis nula donde,

$$
H_{0}: JB\sim \overline{\chi^{2}}_{2 \, g.l.}
$$

```{r Jarque-Bera 3, echo=FALSE}

jarque.bera.test(residuals(linear.house)/50000)
```

El código nos da un resultado en el que, el p-value que tiende a 0, por lo que, con un nivel de significancia del 5% podemos rechazar $H_{0}$.
Por ello concluimos que los residuos no se distribuyen de forma normal.

#### Prueba de Heterocedasticidad (White)

Considere ahora una prueba de Heterocedasticidad, en este caso al ser más de dos variables explicativas se debería aplicar la regresión la prueba de White sin **terminos cruzados**.
Para ello la hipotesis nula se especifica como,

$$
H_{0}: \nexists \, Heterocedasticidad
$$

Corremos la prueba la cual nos arroja el siguiente resultado,

```{r White3, echo=FALSE}
white.house<- as.data.frame(white_lm(linear.house, interactions = FALSE))

stargazer(white.house, type ="text",title = "Cuadro #3.2: Prueba de Heterocedastidad", summary = FALSE)

white.house
```

Observando los resultados de la prueba se obtiene que con un nivel de significancia del 5%, a partir de la prueba de White sin terminos cruzados se obtiene un p-value que tiende a 0 por lo que se rechaza la $H_{0}$ y se confirma la evidencia de heterocedasticidad.

#### Prueba de Multicolinealidad

Inicialmente considere dentro de nuestra correlación si ha de existir un problema de multicolinealidad perfecta.
Corremos un código en el modelo para observar si este detecta algún error.

```{r, echo=TRUE}
lm( PRICE ~ lotsize + bedrooms + bathrooms, data= house, singular.ok = FALSE)
```

Note que por el momento no tenemos un problema de multicolinealidad perfecta.
Sin embargo, es necesario realizar un analisis más de éste.
Considere un análisis de multicolinealidad, para ello considere los siguientes factores, la $R^{2}$ a un nivel de 0.98, vista en el Cuadro \#3; y la matriz de correlaciones entre variables explicativas.

```{r, echo=TRUE}

cor(x=house$lotsize, y= house$bedrooms, method = c("pearson"))
cor(x=house$lotsize, y= house$bathrooms, method = c("pearson"))
cor(x=house$bedrooms, y= house$bathrooms, method = c("pearson"))


```

Visto los resultados de la matriz, vemos que sí existe una correlación entre varias variables.
Por ello ahora usaremos el analisis del indice K.

Procedemos en analsis del indice K donde sabemos que,

$$
K= \frac{\max_{valor \, propio}X'X}{\min_{valor \, propio}X'X}
$$

Donde el Indice K (IC) es $\sqrt{K}$, donde si

-   Si 10 \< IC \< 30 hay multicolinealidad modera fuerte.

-   Si IC\> 30 hay un problema serio de multicolinealidad.

```{r}

#Se crea la matriz de variables explicativas

X <- as.matrix(cbind(house$lotsize, house$bedrooms, house$bathrooms))

#Se computa X'X

X_prima_X <- t(X)%*%X

#Se obtienen los valores propis de la matriz X'X con la función eigen (base de R), se usa la opción only.values = TRUE para que solo brinde los valores propios y no sus vectores asociados
valores_propios<-eigen(X_prima_X, only.values = TRUE)

#Se computa el índice de condición K
indice_condicion_k<-sqrt(max(valores_propios$values))/sqrt(min(valores_propios$values))

indice_condicion_k
```

El Indice K nos da un número extremadamente alto, por lo que se tiene un problema muy serio de multicolinealidad.

#### Autocorrelación

Vemos que, al tener datos de corte transversales y muchas observaciones, el problema de autocorrelación no es uno que nos deba molestar.

### 3.5 Corrección a las Pruebas

Con el fin de remediar la violación de los supuestos, se procede a hacer la siguiente transformación de variables.
El pensamiento original detrás del porqué hacemos esto es porque nuestros datos tienen magnitudes muy grandes y la distribución de baños y habitación siempre de acumula cercano a numero como 1 o 2.
Por ello procedemos con las siguientes

1.  Se obtiene una nueva variable: la proporción de baños sobre habitaciones.
    Es probable esperar que, el número va a ser menor o igual que uno, puesto que, normalmente, se tienen más habitaciones que baños.

    $$
    bathbedratio = \frac{bathrooms}{bedrooms}
    $$

2.  Se obtiene el logaritmo natural de la proporción anteriormente definida.

    $$
    log(bathbedratio) =log( \frac{bathrooms}{bedrooms})
    $$

3.  Se obtiene el logaritmo natural de los precios.

    $$
    log(Price)
    $$

4.  Obtenemos el logaritmo natural del tamaño de los lotes.

    $$
    log(lotsize)
    $$

```{r echo=FALSE, warning=FALSE}
house$bathbedratio_ratio <- house$bathrooms/house$bedrooms
house$log_bathbedratioratio <- log(house$bathbedratio_ratio)
house$log_PRICE <- log(house$PRICE)
house$log_lotsize <- log(house$lotsize)
```

A partir de lo anterior, corremos la siguiente regresión:

```{r, warning = FALSE}
linear.house.corr <- lm( formula = log_PRICE ~ log_lotsize + log_babedratio, data = house)
```

#### Corrigiendo la normalidad.

Vemos que, al aplicar la prueba de Jaque-Bera sobre la regresión transformada, se obtiene que:

```{r Jarque-Bera , echo=FALSE}
jarque.bera.test(residuals(linear.house.corr)/50000)
```

Como ahora se obtiene un P-value mayor que un error de tolerancia de 5%, no se rechaza la hipótesis nula.
Por lo tanto, podemos no rechazar el hecho de que existe homocedasticidad.

#### Corrigiendo la heterocedasticidad.

Si aplicamos la prueba de White sobre la nueva regresión, se obtiene que:

```{r White55, echo=FALSE}
white.house1<- as.data.frame(white_lm(linear.house1, interactions = FALSE))

stargazer(white.house1, type ="text",title = "Cuadro #3.3: Prueba de Heterocedastidad", summary = FALSE)

```

De nuevo, vemos que el nuevo P-value es mayor que el error de tolerancia al 5%, lo que significa que no se rechaza la hipótesis nula.
Por lo tanto, existe homocedasticidad.

#### Corrigiendo la multicolinealidad.

Aplicando nuevamente las pruebas llevadas a cabo anteriormente, se tiene que:

Si buscamos la multicolinealidad perfecta de la nueva regresión, no la vamos a encontrar.
De acuerdo con el siguiente código:

```{r, echo=TRUE}
lm( log_PRICE ~ sqrt_lotsize + log_babedratio, data=house, singular.ok = FALSE)
```

Se observa que no existe una multicolinealidad perfecta entre ambas variables explicativas.

Ahora, si analizamos la correlación que existe entre las variables explicativas se tiene que:

```{r, echo=TRUE}

cor(x=house$sqrt_lotsize, y= house$log_babedratio, method = c("pearson"))

```

Vemos que el coeficiente es menor que 0.8, lo que indica que no se tiene un problema grave de multicolinealidad.

Volvemos a utiilzar el análisis de los valores propios, esta vez, utilizamos el siguiente código:

```{r}

#Se crea la matriz de variables explicativas

X <- as.matrix(cbind(house$log_lotsize, house$log_babedratio))

#Se computa X'X

X_prima_X <- t(X)%*%X

#Se obtienen los valores propis de la matriz X'X con la función eigen (base de R), se usa la opción only.values = TRUE para que solo brinde los valores propios y no sus vectores asociados

valores_propios<-eigen(X_prima_X, only.values = TRUE)

#Se computa el índice de condición K

indice_condicion_k<-sqrt(max(valores_propios$values))/sqrt(min(valores_propios$values))

indice_condicion_k
```

Ahora, vemos que, el IC ahora es menor que 30, sigue persistiendo el problema pero en una menor magnitud.

### 3.6 Comparación

Al comparar ambas regresiones (la original vs la transformada) podemos observar que, la heterocedasticidad en la original es mayor que en la segunda.

```{r, warning = FALSE, echo = FALSE}
stargazer(linear.house, linear.house.corr, type = "text", title = "Cuadro: Comparación de las Regresiones")

```

Y, ahora, gracias a las transformaciones que se le aplicaron a las variables explicativas y dependiente, se logra corregir las violaciones a los supuestos.
En el caso de la multicolinealidad, si bien no se elimina por completo, se logra disminuir la dependencia entre las variables explicativas.
La cual en este caso se justifica ya que la relación de baños, habitaciones y metros cuadrados de las casa esta siempre relacionado.
Ya que entre estas mismas se restringen porque uno no puedo construir muchos cuartos en espacios pequeños y no tiene sentido tener muchos baños para pocas habitaciones.
